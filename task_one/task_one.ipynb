{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e20136e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API Key Loaded. Configuring Gemini...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from dotenv import load_dotenv\n",
    "from google.api_core import exceptions\n",
    "\n",
    "# --- 1. SETUP & CONFIGURATION ---\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# ‚ö†Ô∏è MANUAL KEY OVERRIDE (Only if .env fails)\n",
    "# GOOGLE_API_KEY = \"AIzaSy.....\" \n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    print(\"‚ùå Error: API Key not found. Please check .env or set manually.\")\n",
    "else:\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    print(f\"‚úÖ API Key Loaded. Configuring Gemini...\")\n",
    "\n",
    "# Use Flash model (Fastest/Cheapest)\n",
    "model = genai.GenerativeModel('models/gemini-2.5-flash')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b8a82d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 2: ROBUST API FUNCTIONS (The \"Indefinite Wait\" Fix) ---\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "from google.api_core import exceptions\n",
    "\n",
    "def clean_and_parse_json(response_text):\n",
    "    \"\"\"Clean markdown fences and parse JSON.\"\"\"\n",
    "    try:\n",
    "        text = re.sub(r\"```json|```\", \"\", response_text).strip()\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def get_gemini_response_persistent(prompt, model):\n",
    "    \"\"\"\n",
    "    Retries FOREVER until successful. \n",
    "    If Rate Limit is hit, it pauses and retries.\n",
    "    \"\"\"\n",
    "    wait_time = 10  # Start with 10 seconds\n",
    "    \n",
    "    while True: # Loop forever until success\n",
    "        try:\n",
    "            return model.generate_content(prompt)\n",
    "            \n",
    "        except exceptions.ResourceExhausted:\n",
    "            # We hit the limit. Wait, then retry.\n",
    "            print(f\"      ‚è≥ Limit hit. Waiting {wait_time}s...\", end=\"\\r\")\n",
    "            time.sleep(wait_time)\n",
    "            \n",
    "            # Increase wait time slightly (cap at 60s)\n",
    "            wait_time = min(wait_time * 1.5, 60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ùå Error: {e}\")\n",
    "            return None\n",
    "\n",
    "def run_batch_strategy(df, prompt_func, batch_size=10, strategy_name=\"Strategy\"):\n",
    "    \"\"\"\n",
    "    Process reviews in batches with FORCED delays to stay safe.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüöÄ Starting {strategy_name} (Batch Size: {batch_size})...\")\n",
    "    \n",
    "    batches = [df[i:i + batch_size] for i in range(0, df.shape[0], batch_size)]\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i, batch in enumerate(batches):\n",
    "        print(f\"   üì¶ Batch {i+1}/{len(batches)}...\", end=\" \")\n",
    "        \n",
    "        reviews = batch['text'].tolist()\n",
    "        prompt = prompt_func(reviews)\n",
    "        \n",
    "        # Call the new PERSISTENT function\n",
    "        response = get_gemini_response_persistent(prompt, model)\n",
    "        \n",
    "        batch_preds = [3] * len(reviews)\n",
    "        \n",
    "        if response and response.text:\n",
    "            data = clean_and_parse_json(response.text)\n",
    "            if data and \"ratings\" in data and isinstance(data[\"ratings\"], list):\n",
    "                if len(data[\"ratings\"]) == len(reviews):\n",
    "                    batch_preds = data[\"ratings\"]\n",
    "                    print(\"‚úÖ Success             \") \n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Count Mismatch\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Parse Error\")\n",
    "        else:\n",
    "            print(\"‚ùå API Failed\")\n",
    "            \n",
    "        all_predictions.extend(batch_preds)\n",
    "        \n",
    "        # CRITICAL: Force 10s sleep between batches to stay under the RPM limit\n",
    "        time.sleep(10) \n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f1dbfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Ready: 200 reviews loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. DATA LOADING ---\n",
    "try:\n",
    "    df = pd.read_csv('yelp.csv')\n",
    "    df = df[['text', 'stars']].dropna()\n",
    "    sampled_df = df.sample(n=200, random_state=42).reset_index(drop=True)\n",
    "except (FileNotFoundError, KeyError):\n",
    "    print(\"‚ö†Ô∏è Dataset not found. Generating dummy data for testing.\")\n",
    "    data = {\n",
    "        'text': [\n",
    "            \"The food was absolutely terrible and cold.\", \n",
    "            \"Amazing experience! Loved the ambiance.\",\n",
    "            \"It was okay, nothing special.\", \n",
    "            \"Service was slow but burger was tasty.\", \n",
    "            \"Worst place I have ever been to.\"\n",
    "        ] * 40, \n",
    "        'stars': [1, 5, 3, 4, 1] * 40\n",
    "    }\n",
    "    sampled_df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"‚úÖ Data Ready: {len(sampled_df)} reviews loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "044d2413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. PROMPT DEFINITIONS ---\n",
    "\n",
    "def prompt_batch_zero(reviews_list):\n",
    "    formatted = \"\\n\".join([f\"Review {i+1}: {r}\" for i, r in enumerate(reviews_list)])\n",
    "    return f\"\"\"\n",
    "    You are a sentiment analyzer. Classify these {len(reviews_list)} reviews (1-5 stars).\n",
    "    Reviews:\n",
    "    {formatted}\n",
    "    Output STRICTLY a JSON object with a single list of integers:\n",
    "    {{ \"ratings\": [star_1, star_2, star_3, ...] }}\n",
    "    \"\"\"\n",
    "\n",
    "def prompt_batch_few(reviews_list):\n",
    "    formatted = \"\\n\".join([f\"Review {i+1}: {r}\" for i, r in enumerate(reviews_list)])\n",
    "    return f\"\"\"\n",
    "    Classify these Yelp reviews (1-5 stars) based on examples:\n",
    "    Example 1: \"Rude service, cold food.\" -> 1\n",
    "    Example 2: \"Okay meal, overpriced.\" -> 3\n",
    "    Example 3: \"Delicious! Great staff.\" -> 5\n",
    "    \n",
    "    Now classify these new reviews:\n",
    "    {formatted}\n",
    "    Output STRICTLY a JSON object with a single list of integers:\n",
    "    {{ \"ratings\": [star_1, star_2, star_3, ...] }}\n",
    "    \"\"\"\n",
    "\n",
    "def prompt_batch_cot(reviews_list):\n",
    "    formatted = \"\\n\".join([f\"Review {i+1}: {r}\" for i, r in enumerate(reviews_list)])\n",
    "    return f\"\"\"\n",
    "    Analyze the following reviews step-by-step.\n",
    "    For each review, briefly weigh pros/cons and assign a score (1-5).\n",
    "    \n",
    "    Reviews:\n",
    "    {formatted}\n",
    "    \n",
    "    Output STRICTLY a JSON object containing a list of integers:\n",
    "    {{ \n",
    "      \"reasoning\": \"Brief summary...\",\n",
    "      \"ratings\": [star_1, star_2, star_3, ...] \n",
    "    }}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44f6e078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting Zero-Shot (Batch Size: 10)...\n",
      "      ‚è≥ Limit hit. Waiting 60s...s.... Waiting 10s...\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mget_gemini_response_persistent\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.ResourceExhausted:\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# We hit the limit. Wait, then retry.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/google/generativeai/generative_models.py:331\u001b[39m, in \u001b[36mGenerativeModel.generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/google/api_core/retry/retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:77\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250, model: gemini-2.5-flash\nPlease retry in 45.232848151s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 250\n}\n, retry_delay {\n  seconds: 45\n}\n]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- CELL 5: RUN ALL STRATEGIES ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 1. Run Zero-Shot (Batch Size 10)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m sampled_df[\u001b[33m'\u001b[39m\u001b[33mpred_zero_shot\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mrun_batch_strategy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampled_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_batch_zero\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mZero-Shot\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 2. Run Few-Shot (Batch Size 10)\u001b[39;00m\n\u001b[32m      9\u001b[39m sampled_df[\u001b[33m'\u001b[39m\u001b[33mpred_few_shot\u001b[39m\u001b[33m'\u001b[39m] = run_batch_strategy(\n\u001b[32m     10\u001b[39m     sampled_df, prompt_batch_few, batch_size=\u001b[32m10\u001b[39m, strategy_name=\u001b[33m\"\u001b[39m\u001b[33mFew-Shot\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mrun_batch_strategy\u001b[39m\u001b[34m(df, prompt_func, batch_size, strategy_name)\u001b[39m\n\u001b[32m     52\u001b[39m prompt = prompt_func(reviews)\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Call the new PERSISTENT function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m response = \u001b[43mget_gemini_response_persistent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m batch_preds = [\u001b[32m3\u001b[39m] * \u001b[38;5;28mlen\u001b[39m(reviews)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mand\u001b[39;00m response.text:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mget_gemini_response_persistent\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.ResourceExhausted:\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# We hit the limit. Wait, then retry.\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m      ‚è≥ Limit hit. Waiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms...\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# Increase wait time slightly (cap at 60s)\u001b[39;00m\n\u001b[32m     33\u001b[39m     wait_time = \u001b[38;5;28mmin\u001b[39m(wait_time * \u001b[32m1.5\u001b[39m, \u001b[32m60\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- CELL 5: RUN ALL STRATEGIES ---\n",
    "\n",
    "# 1. Run Zero-Shot (Batch Size 10)\n",
    "sampled_df['pred_zero_shot'] = run_batch_strategy(\n",
    "    sampled_df, prompt_batch_zero, batch_size=10, strategy_name=\"Zero-Shot\"\n",
    ")\n",
    "\n",
    "# 2. Run Few-Shot (Batch Size 10)\n",
    "sampled_df['pred_few_shot'] = run_batch_strategy(\n",
    "    sampled_df, prompt_batch_few, batch_size=10, strategy_name=\"Few-Shot\"\n",
    ")\n",
    "\n",
    "# 3. Run Chain-of-Thought (Batch Size 5)\n",
    "# We use smaller batches (5) because CoT generates more text\n",
    "sampled_df['pred_cot'] = run_batch_strategy(\n",
    "    sampled_df, prompt_batch_cot, batch_size=5, strategy_name=\"Chain-of-Thought\"\n",
    ")\n",
    "\n",
    "print(\"\\nüéâ ALL EXPERIMENTS COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53364c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. RESULTS & VISUALIZATION ---\n",
    "\n",
    "# Calculate Accuracy\n",
    "acc_zero = accuracy_score(sampled_df['stars'], sampled_df['pred_zero_shot'])\n",
    "acc_few = accuracy_score(sampled_df['stars'], sampled_df['pred_few_shot'])\n",
    "acc_cot = accuracy_score(sampled_df['stars'], sampled_df['pred_cot'])\n",
    "\n",
    "# Create Summary DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Strategy': ['Zero-Shot', 'Few-Shot', 'Chain-of-Thought'],\n",
    "    'Accuracy': [acc_zero, acc_few, acc_cot]\n",
    "})\n",
    "\n",
    "print(\"\\n--- üèÜ FINAL RESULTS SUMMARY ---\")\n",
    "print(results_df)\n",
    "\n",
    "# Plot Confusion Matrix (Chain of Thought)\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(sampled_df['stars'], sampled_df['pred_cot'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix: Chain-of-Thought')\n",
    "plt.show()\n",
    "\n",
    "# Save File\n",
    "sampled_df.to_csv(\"yelp_final_results.csv\", index=False)\n",
    "print(\"‚úÖ Results saved to 'yelp_final_results.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
